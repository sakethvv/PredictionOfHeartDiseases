---
title: "R Notebook"
output:
  word_document: default
  html_notebook: default
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
library(mice)
library(naniar)
library(VIM)
library(DataExplorer)
library(caret)
library(dplyr)
library(plyr)
library(Hmisc)
library(car)
library(MASS)
library(MLmetrics)
origindal_data <- read.csv("C:/Users/Dr. Suresh babu/Desktop/Data Mining Project/heart_disease_entire_data.csv")

vis_miss(origindal_data)


cat_imputed_data <- read.csv("C:/Users/Dr. Suresh babu/Desktop/Data Mining Project/categorical values imputed.csv")

head(cat_imputed_data)
md.pattern(cat_imputed_data)

md.pairs(cat_imputed_data)

str(cat_imputed_data)
cat_imputed_data$sex <- as.factor(cat_imputed_data$sex)
cat_imputed_data$cp <- as.factor(cat_imputed_data$cp)
cat_imputed_data$fbs <- as.factor(cat_imputed_data$fbs)
cat_imputed_data$restecg <- as.factor(cat_imputed_data$restecg)
cat_imputed_data$exang <- as.factor(cat_imputed_data$exang)
cat_imputed_data$slop<- as.factor(cat_imputed_data$slop)
cat_imputed_data$ca <- as.factor(cat_imputed_data$ca)
cat_imputed_data$thal <- as.factor(cat_imputed_data$thal)
cat_imputed_data$pred_attribute <- as.factor(cat_imputed_data$pred_attribute)

imp2 <- mice(cat_imputed_data, m=3, seed =12456)
imp2

imp2$imp$trestbps

cat_imputed_data[1,]
imp_tot <- complete(imp2, 1)
head(imp_tot)

write.csv(imp_tot, file = "C:/Users/Dr. Suresh babu/Desktop/Data Mining Project/final imputed.csv")

imp_tot$pred_attribute <- as.factor(imp_tot$pred_attribute)
str(imp_tot)

numeric_data <- imp_tot
numeric_data$sex <- as.integer(numeric_data$sex)
numeric_data$cp <- as.integer(numeric_data$cp)
numeric_data$fbs <- as.integer(numeric_data$fbs)
numeric_data$restecg <- as.integer(numeric_data$restecg)
numeric_data$exang <- as.integer(numeric_data$exang)
numeric_data$slop<- as.integer(numeric_data$slop)
numeric_data$ca <- as.integer(numeric_data$ca)
numeric_data$thal <- as.integer(numeric_data$thal)
numeric_data$pred_attribute <- as.numeric(numeric_data$pred_attribute)

plot_correlation(numeric_data)
plot_correlation(imp_tot)

summary(duplicated(imp_tot))

imp_tot[duplicated(imp_tot),]
  
unique(imp_tot)

head(imp_tot)

#Naive's rule  - incomplete
table(imp_tot$pred_attribute)

#Logistic regression
logistic_heart_data <- imp_tot


set.seed(123)
train.index <- logistic_heart_data$pred_attribute %>% createDataPartition(p =0.7,list =FALSE)
train.data <- logistic_heart_data[train.index,]
test.data <- logistic_heart_data[-train.index,]

describe(train.data)
describe(test.data)

#Test for multicollinearity
vif(glm(pred_attribute~.,data =train.data,family =binomial))
 # the results show that the values for all the variables is between 1-2 which means that there is no collinearity between the variables
  
logit_mod <- glm(pred_attribute~.,data =train.data,family =binomial)

summary(logit_mod)


fitted.results.logit <- logit_mod %>% predict(test.data,type ="response")
predicted.class.logit <- ifelse(fitted.results.logit>0.5,1,0)
misclassificationError <- mean(predicted.class.logit !=test.data$pred_attribute)
print(paste('Accuracy of Logistic Regression is', 1- misclassificationError))
# Logistic regression with a cut off of 0.5 gave an accuracy of 0.91640866873065


#stepwise logistic regression in R - backwards

step.model.logit <- stepAIC(logit_mod,trace = TRUE)
summary(step.model.logit)
predicted.stepwise.logit <- step.model.logit %>% predict(test.data,type ="response")
predicted.prob.stepwise.logit <- ifelse(predicted.stepwise.logit>0.5,1,0) 
misclassificationErrorStepWise <- mean(predicted.prob.stepwise.logit != test.data$pred_attribute)
print(paste('Accuracy of Logistic Regression is', 1- misclassificationErrorStepWise))

# THis shows that the model gives the same accuracy as the complete even  by removing the variables thalach and chol, therefore we dont need to have these two columns



#cross validation 

train_control <- trainControl(method = "cv", number = 10)
linearCrossMod <- train(pred_attribute ~ age+sex+ cp + trestbps + chol+fbs+restecg+thalach+exang+oldpeak+ca+thal, 
               data=train.data, 
               trControl = train_control,
               method = "glm",
               family=binomial())

summary(linearCrossMod)
#Residual deviance of 319.52 is much lesser than the null deviance 1050.19 , which shows that fit is good
linearCrossPred <- predict(linearCrossMod, test.data, type='raw')
linearCrossPredCorrect <- data.frame(target=test.data$pred_attribute, predicted=linearCrossPred, match=(test.data$pred_attribute == linearCrossPred))
print(length(linearCrossPredCorrect$match[linearCrossPredCorrect$match==TRUE])/nrow(test.data))

#the accuracy achieved through cross validation is also similar which proves that the model built is a good fit.

#ROC and choosing the best cut off 

res <- predict(step.model.logit,type ="response")
ROCR_Pred <- prediction(res,train.data$pred_attribute)
ROCR_perf <- performance(ROCR_Pred,"tpr","fpr")
plot(ROCR_perf,colorize=T,print.cutoffs.at =seq(0.1,by =0.1))

as.numeric(performance(ROCR_Pred, 'auc')@y.values)
#the AOC is 0.9737538 which confirms the high predictive capbility of the model, we can also see that the ideal cutoff value is around 0.3 and therefore we now classify the predicted values with this cutoff
#cut off is around 0.3
final.model <- glm(pred_attribute~.,data =train.data,family =binomial) %>%stepAIC(trash =FALSE)
prob.final <- predict(final.model,test.data,type ="response")
predicted.class_final <- ifelse(prob.final>0.3,1,0)
table(test.data$pred_attribute, predicted.class_final == '1',dnn = c("Actual", "Predicted"))



finalMisclassificationError <- mean(predicted.class_final!=test.data$pred_attribute)
print(paste('Accuracy of Logistic Regression is', 1- finalMisclassificationError))

#doing this increased our accuracy slightly to 0.919504643962848

F1_Score(y_pred = predicted.class_final, y_true = test.data$pred_attribute, positive = "0")
F1_Score(y_pred = predicted.class_final, y_true = test.data$pred_attribute, positive = "1")

#F1 scores for both the classification groups is around 91 to 92% which is a really good score

#dataquality library
#find R square value
library(rsq)
rsq(step.model.logit,adj=FALSE)
rsq(step.model.logit,adj=TRUE)

#R^2 value of 0.7759015 is achieved for the model and the adj R^2 is 0.7698284 which are pretty close to each other thereby proving the number of predictors considered are good

#2 Tree model
library(rpart)
library(rpart.plot)
tree = rpart(pred_attribute~., data=train.data, method='class')

prp(tree)
predictTree = predict(tree, newdata=test.data, type='class')

table(test.data$pred_attribute, predictTree, dnn = c("Actual", "Predicted"))

#Accuracy
(141+154)/nrow(test.data)
#An accuracy of 0.9133127 is achieved using the decision trees which is less than 0.3 of logistic regression

F1_Score(y_pred = predictTree, y_true = test.data$pred_attribute, positive = "1")


predictTree = predict(tree, newdata=test.data)

ROCRtree = prediction(predictTree[,2],test.data$pred_attribute)

ROCRperf1 = performance(ROCRtree, 'tpr','fpr')
plot(ROCRperf1,colorize=TRUE)


#AUC
as.numeric(performance(ROCRtree, 'auc')@y.values)
#the AOC is 0.9541138, though a very good score it is still less compared to the AOC achieved by logistic regression. ALso we can see that a cutoff of 0.2 might lead to better results , therefore we classify once again using this cutoff 


#overlay of both the ROC curves
plot(ROCR_perf, col = 1, lty = 2, main = "ROC")
plot(ROCRperf1, col = 4, lty = 3, add = TRUE)
legend("bottomright", c("Logistic Regression", "Deecision Trees"), bty="n", lty=c(2,3), lwd=2, col = c(1,4),  title = "ROC of both the models")

#overlapping the two ROC we can see that the curve obtained by logistic regression is slightly better than that achieved by the decision trees
```




